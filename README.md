

# LIPE: Learning Personalized Identity Prior for Non-rigid Image Editing

![teaser](figures/teaser.png)

[![Paper](https://img.shields.io/badge/cs.CV-Paper-66cdaa?logo=arxiv&logoColor=66cdaa)](https://arxiv.org/abs/2406.17236)
[![Project Page](https://img.shields.io/badge/Project-Website-66cdaa?logo=googlechrome&logoColor=66cdaa)](https://ay-liu.github.io/lipe.github.io/)

> Aoyang Liu<sup>1</sup>, Qingnan Fan<sup>2</sup>, Shuai Qin<sup>2</sup>, Hong Gu<sup>2</sup>, Yansong Tang<sup>1</sup>
> 
> <sup>1</sup>Tsinghua University, <sup>2</sup>VIVO.

>**Abstract**: 
> Although recent years have witnessed significant advancements in image editing thanks to the remarkable progress of text-to-image diffusion models, the problem of non-rigid image editing still presents its complexities and challenges. Existing methods often fail to achieve consistent results due to the absence of unique identity characteristics. In this paper, we present LIPE, a novel framework designed to learn the personalized identity prior from a limited set of images of the same subject, and subsequently employ the learned prior for non-rigid image editing. Our approach comprises two functional designs, EDiting-Oriented personalized identity Prior (EDOP) and Non-rigid Image editing via identity-aware MAsk blend (NIMA), to address the prior learning and image editing challenges, respectively. Experimental results demonstrate the advantages of our approach in various editing scenarios over the past related leading methods in both qualitative and quantitative ways.



